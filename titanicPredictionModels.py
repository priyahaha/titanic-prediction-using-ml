# -*- coding: utf-8 -*-
"""Copy of titanicV1

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uHLegrNGqi8wJlZZGAsITh5SDe0OJ7jj
"""

#Import the libraries required
import numpy as np
import pandas as pd
import os
import warnings

#Importing the Machine Learning libraries
import sklearn.metrics as metrics
from scipy.stats import uniform
from scipy.stats import randint as sp_randint
from sklearn.model_selection import train_test_split
from sklearn import tree
from sklearn.model_selection import RandomizedSearchCV
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.gaussian_process import GaussianProcessClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import SGDClassifier
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

#This line does not print the warnings in the code
warnings.simplefilter('ignore')
#Get the current folder path
currentFolder = os.getcwd()
#Get the location of training dataset file
trainFilePath = os.path.join(currentFolder, 'train.csv')
#Get the location of testing dataset file
testFilePath = os.path.join(currentFolder, 'test.csv')

#This function reads the Training Dataset into a Pandas Dataframe
def getTrainingDataset():
  return pd.read_csv(trainFilePath)

#This function does Data Pre-processing, filling all the empty cells converting it into standardized format
def cleanFeatureData(X):
  X["Age"] = X["Age"].fillna(X["Age"].value_counts().idxmax()) #Fills all empty cells with the most common value of age
  X["Sex"] = X["Sex"].replace({'male': 0, 'female': 1}) #Converts the data into binary format
  X['Cabin'] = X["Cabin"].apply(lambda x: 0 if type(x) == float else 1) #Check if the passenger has a cabin or not
  X["Embarked"] = X["Embarked"].fillna(X["Embarked"].value_counts().idxmax()) #Fills all empty cells with the most common value of port
  X['Fare'].fillna(X["Fare"].value_counts().idxmax(), inplace=True) #Fills all empty cells with the most common value fare
  return X

#This function mines information and patters from the data into improvised features
def addMinedFeatures(X):
  X = cleanFeatureData(X)
  X["FamilySizeRange"] = pd.cut( (X["SibSp"] + X["Parch"] + 1), bins=[0, 2, 5, 15], labels=[0, 1, 2]) #Converts Family Size Range into different label classes
  X["Title"] = X["Name"].str.extract(" ([A-Za-z]+)\.") #Extracts title from the Name
  X['Title'] = X['Title'].replace({'Mr': 1, 'Miss': 2, 'Mrs': 3, 'Master': 4, 'Dr': 5, 'Rev': 5, 'Col': 5, 
                                     'Major': 5, 'Mlle': 5, 'Countess': 5, 'Ms': 2, 'Lady': 5, 'Jonkheer': 5, 
                                     'Don': 5, 'Dona': 5, 'Mme': 5, 'Capt': 5, 'Sir': 5, 'the': 5}) #Assigns labels to each title based on the importance
  X["Single"] = X["FamilySizeRange"].apply(lambda x: 1 if x == 1 else 0) #Checks if the passenger is Single or not
  X["FareRange"] = pd.cut(X["Fare"], bins=[0, 10, 15, 31, 120, 300], labels=[0, 1, 2, 3, 4]) #Converts Family Size Range into different label classes
  X['FareRange'].fillna(X["FareRange"].value_counts().idxmax(), inplace=True) #Converts Fare Range into different label classes
  X["AgeRange"] = pd.cut(X["Age"], bins=[0, 18, 25, 35, 60, 100], labels=[0, 1, 2, 3, 4]) #Converts Age Range into different label classes
  X["Embarked"] = X["Embarked"].replace({'S': 0, 'C': 1, 'Q': 2}) #Converts the Embarked ports data into numerical format
  X['FareAndAge'] = X["Fare"] * X["Age"] #Creates a new feature which is the product of Fare and Age

  if "Survived" in X.columns:
    X = X.drop(['Survived', 'PassengerId', 'Name', 'Ticket', 'Age', 'Fare'], axis=1) #Removes all the old columns from the dataset
  else:
    X = X.drop(['PassengerId', 'Name', 'Ticket', 'Age', 'Fare'], axis=1) #The same function is used to transform testing data, hence, this line drops all the unnessecary coloumnns from test dataset

  #Return final dataset
  return X

#This function returns the Training Dataset after feature mining
def getMinedTrainingDataset():
  X = addMinedFeatures(getTrainingDataset()) #Call the function to mine features
  y = getTrainingDataset()["Survived"] #Gets output values for supervised training
  return X,y

#This function returns the Testing Dataset after feature mining for Kaggle Output
def getMinedTestDataset():
  XTest = pd.read_csv(testFilePath) #Reads the Testing dataset
  pId = pd.get_dummies(XTest[["PassengerId"]]) #Parses Passenger Ids
  XTest = addMinedFeatures(XTest)  #Call the function to mine features
  XTest.to_csv('FinalDataset'+'.csv', index=False) #Save the file for future refrence
  return XTest, pId

#Predicts outputs for the given model and returns it as a dataframe
def predictOutput(model, XTest, pId):
  predictions = model.predict(XTest) #Predicts data from model for XTest values
  output = pd.DataFrame({'PassengerId': pId.PassengerId, 'Survived': predictions}) #Parses data in the format required by kaggle
  return output

#Downloads the output with the given filename
def downloadOutput(output, filename):
  output.to_csv(filename+'.csv', index=False) #Saves file to your local

#Splits the Training Dataset with the given partition as Training and Testing
def splitDataset(size):
  X, y = getMinedTrainingDataset() #Gets the Training Data
  return train_test_split(X, y, test_size=size, random_state=42) # Returns the splitted X_train, X_test, y_train, y_test data

#Prints Confusion Matrix for the Model
def showConfusionMatrix(y_test, y_pred, model):
  cMatrix = confusion_matrix(y_test, y_pred, labels=model.classes_) #Buils Confusion Matrix object for the given values
  disp = ConfusionMatrixDisplay(confusion_matrix=cMatrix,display_labels=model.classes_) #Displays matrix
  disp.plot()
  plt.show() #Prints the matrix to output

#Calculates the metrics for the model
def calculateMetrics(y_test, y_pred, model):
  print('Accuracy Score   : %.2f' % metrics.accuracy_score(y_test,y_pred)) #Calculates Accuracy
  print('F1 Score         : %.2f' % metrics.f1_score(y_test, y_pred)) #Calculates F1 Score
  print('Precision Score  : %.2f' % metrics.precision_score(y_test, y_pred)) #Calculates Precision
  print('Recall Score     : %.2f' % metrics.recall_score(y_test, y_pred)) #Calculates Recall
  print('ROC AUC Score    : %.2f' % metrics.roc_auc_score(y_test, y_pred)) #Calculates ROC AUC Score
  showConfusionMatrix(y_test, y_pred, model) #Prints the Confusion Matrix

#Perfoms Randomized Search to get the best parameters
def doRandomizedSearch(model, parameters):
  X, y = getMinedTrainingDataset() #Gets the Training Data
  rsModel = RandomizedSearchCV(estimator=model, param_distributions=parameters, n_iter=10, cv=5, random_state=42) #Call the Randomizer with the parameters
  rsModel.fit(X, y) #Fits the model
  print("Best Parameters for ", model.__class__.__name__ +" are \n", rsModel.best_params_) #Prints the best Parameters
  return rsModel.best_params_ #Returns the best parameters

#Run the Machine Learning Models
def runClassifierModel(datsetSplitValue,model, download):
  print("\n"+ model.__class__.__name__ + "\n") #Print Model Name
  X_train, X_test, y_train, y_test = splitDataset(datsetSplitValue) #Split the dataset into Training and Testing 
  model.fit(X_train, y_train) #Fit the Model with Training Data
  y_pred = model.predict(X_test) #Predict the ouputs for Test Data
  calculateMetrics(y_test, y_pred, model) #Calculate metrics for the model
  if(download): #Download the predictions file
    XTest, pId = getMinedTestDataset() #Get the kaggle testing dataset
    downloadOutput(predictOutput(model, XTest, pId), str(model.__class__.__name__)) #Predict and download the ouput

#Define the Parameters for each Model based on which Randomized Search CV is run and picks the best

parametersForRFC = {
                'n_estimators': sp_randint(10, 1000),
                'max_depth': sp_randint(2, 100),
                'min_samples_split': sp_randint(2, 100),
                'min_samples_leaf': sp_randint(1, 100),
                'max_features': np.arange(0.1, 1.0, 0.1),
                'random_state': sp_randint(5, 500)
                }

parametersForDTC = {
                'criterion' : ['gini', 'entropy', 'log_loss'],
                'max_depth': sp_randint(2, 100),
                'min_samples_split': sp_randint(2, 100),
                'min_samples_leaf': sp_randint(1, 100),
                'max_features': np.arange(0.1, 1.0, 0.1),
                'random_state': sp_randint(5, 500)
                }

parametersForKNN = {
                  'n_neighbors': sp_randint(1,10),
                  'weights': ['uniform', 'distance'],
                  'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],
                  'leaf_size': sp_randint(10,100),
                  'p': sp_randint(1,50)
                }

parametersForSGD = {
                  'loss': ['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron', 'squared_loss'],
                  'penalty': ['l2', 'l1', 'elasticnet'],
                  'alpha': [0.0001, 0.001, 0.01, 0.1],
                  'l1_ratio': [0, 0.25, 0.5, 0.75, 1],
                  'learning_rate': ['constant', 'optimal', 'invscaling', 'adaptive'], 
                  'max_iter': sp_randint(1000,10000)
                 }
          
parametersForGPC = {
                  "optimizer": ["fmin_l_bfgs_b"],
                  "n_restarts_optimizer": [5, 10, 15, 20],
                  "max_iter_predict": [100, 200, 300, 400, 500],
                  "warm_start": [True, False]
                }

#Call Randomized Search for Various Models with the Parameters
RFCBestParameters = doRandomizedSearch(RandomForestClassifier(), parametersForRFC)
DTCBestParameters = doRandomizedSearch(tree.DecisionTreeClassifier(), parametersForDTC)
KNNBestParameters = doRandomizedSearch(KNeighborsClassifier(), parametersForKNN)
SGDBestParameters = doRandomizedSearch(SGDClassifier(), parametersForSGD)
GPCBestParameters = doRandomizedSearch(GaussianProcessClassifier(), parametersForGPC)

#Train and Predict using Various Classifer Models
runClassifierModel(0.3, KNeighborsClassifier(**KNNBestParameters), False)
runClassifierModel(0.3, tree.DecisionTreeClassifier(**DTCBestParameters), False)
runClassifierModel(0.3, SGDClassifier(**SGDBestParameters), False)
runClassifierModel(0.3, GaussianProcessClassifier(**GPCBestParameters), False)
print('\nBest Classifier Model & Metrics:')
runClassifierModel(0.3, RandomForestClassifier(**RFCBestParameters), True) #Here, we are downloading only the best classifier output